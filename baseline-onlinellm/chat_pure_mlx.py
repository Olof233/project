import json
import os
import time
import math
import multiprocessing
import re
from mlx_lm import load, generate

# ================= é…ç½®åŒºåŸŸ =================
INPUT_FILE = 'data_clean/questions/Mainland/test.jsonl'
OUTPUT_FILE = './test_results_norag_parallel.jsonl'
MODEL_ID = "Qwen/Qwen3-0.6B-MLX-4bit"

# M2 Max 32G æ¨èé…ç½®
NUM_WORKERS = 16  
BATCH_SIZE = 24   

# ç³»ç»Ÿç¯å¢ƒä¼˜åŒ–
os.environ["MTL_COMPUTE_PREF"] = "high-performance"  
os.environ["MLX_GPU_MEMORY_LIMIT"] = "28GB"          
os.environ["TOKENIZERS_PARALLELISM"] = "false"       

# ğŸ”¥ ä¿®æ”¹1ï¼šPrompt ä¼˜åŒ– - ä½¿ç”¨ One-Shot (å•ä¾‹) å¼•å¯¼ï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬çš„ /no_think
# 0.6B å°æ¨¡å‹éœ€è¦â€œçœ‹ä¾‹å­â€æ‰èƒ½æ‡‚ï¼Œè€Œä¸æ˜¯â€œå¬å‘½ä»¤â€
TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦ä¸“å®¶ã€‚è¯·é˜…è¯»é¢˜ç›®å’Œé€‰é¡¹ï¼Œç›´æ¥é€‰å‡ºæœ€æ­£ç¡®çš„ä¸€é¡¹ã€‚

ã€ç¤ºä¾‹ã€‘
é¢˜ç›®ï¼šæ„Ÿå†’çš„å¸¸è§ç—‡çŠ¶ä¸åŒ…æ‹¬ï¼Ÿ
é€‰é¡¹ï¼šA. é¼»å¡ B. å’³å—½ C. éª¨æŠ˜ D. å‘çƒ­ E. ä¹åŠ›
ç­”æ¡ˆï¼šC

ã€æ­£å¼é¢˜ç›®ã€‘
é¢˜ç›®ï¼š{question}
é€‰é¡¹ï¼š{options}
ç­”æ¡ˆï¼š"""

def log(message, worker_id=None):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è¾“å‡º"""
    prefix = f"[Worker {worker_id}]" if worker_id is not None else "[Main]"
    print(f"{time.strftime('%H:%M:%S')} {prefix} {message}")

def calculate_accuracy(file_path):
    """å†…ç½®å‡†ç¡®ç‡è®¡ç®—å‡½æ•°"""
    print("\n" + "="*50)
    log(f"Starting Accuracy Calculation for {file_path}...")
    
    total = 0
    correct = 0
    valid_format = 0
    
    if not os.path.exists(file_path):
        log(f"Error: Result file {file_path} not found.")
        return

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip(): continue
            try:
                item = json.loads(line)
                total += 1
                
                # 1. è·å–æ¨¡å‹å“åº”
                response = item.get('response', '').strip()
                
                # 2. è·å–çœŸå®æ ‡ç­¾ (å…¼å®¹å¤šç§å­˜å‚¨æ ¼å¼)
                # ğŸ”¥ ä¿®å¤ï¼šå¢å¼ºçš„ Ground Truth æå–é€»è¾‘
                raw_answer = item.get('answer', [])
                ground_truth = "N/A"
                
                if isinstance(raw_answer, list) and len(raw_answer) > 0:
                    # ä¼˜å…ˆå– answer_idx (A/B/C/D)
                    ground_truth = str(raw_answer[0]).strip()
                else:
                    # å¦‚æœåªæœ‰æ–‡æœ¬ï¼Œå°è¯•ä» meta_info æˆ–å…¶ä»–åœ°æ–¹æ‰¾ï¼Œæˆ–è€…æš‚æ—¶åªç»Ÿè®¡æ ¼å¼
                    ground_truth = str(raw_answer).strip()

                # 3. æ­£åˆ™æå–æ¨¡å‹è¾“å‡ºçš„é€‰é¡¹
                match = re.search(r'([A-E])', response.split('\n')[0]) 
                
                if match:
                    pred = match.group(1)
                    valid_format += 1
                    # åªæœ‰å½“ ground_truth ä¹Ÿæ˜¯ A-E å•å­—æ¯æ—¶ï¼Œæ¯”å¯¹æ‰æœ‰æ„ä¹‰
                    if pred == ground_truth:
                        correct += 1
                
            except Exception as e:
                # print(f"Error parsing line: {e}") # å‡å°‘åˆ·å±
                pass

    # 3. è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
    if total > 0:
        acc = (correct / total) * 100
        print("-" * 30)
        print(f"ğŸ“Š Evaluation Report:")
        print(f"   Total Samples:   {total}")
        print(f"   Valid Responses: {valid_format} (Format Compliance: {valid_format/total*100:.1f}%)")
        print(f"   Correct:         {correct}")
        print(f"   Wrong:           {total - correct}")
        print(f"   âœ… Accuracy:      {acc:.2f}%")
        print("-" * 30)
    else:
        print("âš ï¸  No data found in result file.")

def worker_task(worker_id, data_chunk):
    """å­è¿›ç¨‹ï¼šæ‰§è¡Œæ¨ç†ä»»åŠ¡"""
    log("Loading model...", worker_id)
    model, tokenizer = load(MODEL_ID, tokenizer_config={"trust_remote_code": True})
    
    # é¢„çƒ­
    dummy_msg = [{"role": "user", "content": "A"}]
    # å°è¯•å…³é—­ thinking (å¦‚æœæ”¯æŒ)ï¼Œä¸æ”¯æŒåˆ™å¿½ç•¥
    try:
        dummy_prompt = tokenizer.apply_chat_template(dummy_msg, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    except TypeError:
        dummy_prompt = tokenizer.apply_chat_template(dummy_msg, tokenize=False, add_generation_prompt=True)
    
    generate(model, tokenizer, prompt=dummy_prompt, max_tokens=2, verbose=False)
    log("Model ready âœ“", worker_id)
    
    results = []
    processed = 0
    total_items = len(data_chunk)
    
    for i in range(0, total_items, BATCH_SIZE):
        batch = data_chunk[i:i + BATCH_SIZE]
        
        for item in batch:
            try:
                # æ„é€  Prompt
                prompt_content = TEMPLATE.format(question=item['question'], options=item['options'])
                messages = [{"role": "user", "content": prompt_content}]
                
                # Tokenizer å¤„ç†
                try:
                    prompt_text = tokenizer.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True,
                        enable_thinking=False 
                    )
                except TypeError:
                    prompt_text = tokenizer.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                

                response = generate(
                    model, 
                    tokenizer, 
                    prompt=prompt_text, 
                    max_tokens=64,
                    verbose=False,
                )
                
                results.append({
                    'id': item.get('id'),
                    'response': response.strip(),
                    # ğŸ”¥ ä¿®æ”¹3ï¼šç¡®ä¿ä¿å­˜ç»“æ„ä¸º [answer_idx, answer_text]
                    # ä¼˜å…ˆå– item['answer_idx']ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å– item['answer'] çš„ç¬¬ä¸€ä¸ªå…ƒç´ (å¦‚æœæ˜¯åˆ—è¡¨)
                    'answer': [item.get('answer_idx', item.get('answer')), item.get('answer')], 
                    'meta_info': item.get('meta_info')
                })
                
            except Exception as e:
                log(f"Error: {e}", worker_id)
                results.append({'id': item.get('id'), 'response': "ERROR", 'answer': []})

        processed += len(batch)
        if worker_id == 1:
            print(f"\rProgress: [{processed}/{total_items}]", end="")
            
    return results

def main():
    multiprocessing.set_start_method('spawn', force=True)
    
    log(f"Loading data from {INPUT_FILE}...")
    if not os.path.exists(INPUT_FILE):
        log(f"Error: {INPUT_FILE} not found.")
        return
        
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        questions = [json.loads(line) for line in f if line.strip()]
    
    total_items = len(questions)
    log(f"Total items: {total_items}. Launching {NUM_WORKERS} workers...")
    
    chunk_size = math.ceil(total_items / NUM_WORKERS)
    chunks = [questions[i:i + chunk_size] for i in range(0, total_items, chunk_size)]
    tasks = [(i+1, chunk) for i, chunk in enumerate(chunks)]
    
    start_time = time.time()
    
    with multiprocessing.Pool(processes=NUM_WORKERS) as pool:
        results_nested = pool.starmap(worker_task, tasks)
    
    final_results = [item for sublist in results_nested for item in sublist]
    
    log(f"Saving {len(final_results)} results to {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for res in final_results:
            f.write(json.dumps(res, ensure_ascii=False) + '\n')
            
    total_time = time.time() - start_time
    
    print("\n" + "="*50)
    print(f"â±ï¸  Inference Completed in {total_time:.2f}s")
    print(f"âš¡ Throughput: {total_items / total_time:.2f} items/sec")
    
    calculate_accuracy(OUTPUT_FILE)

if __name__ == "__main__":
    main()