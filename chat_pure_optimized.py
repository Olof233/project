import json
import os
import time
import math
import multiprocessing
from mlx_lm import load, generate

# ================= é…ç½®åŒºåŸŸ =================
INPUT_FILE = 'data_clean/questions/Mainland/test.jsonl'
OUTPUT_FILE = './test_results_norag_parallel.jsonl'
# ä¿æŒå’Œä½ å•è¿›ç¨‹ä»£ç ä¸€è‡´çš„æ¨¡å‹ ID
MODEL_ID = "Qwen/Qwen3-0.6B-MLX-4bit"

# è¿›ç¨‹æ•°ï¼šM2 Max å»ºè®®è®¾ç½®ä¸º 8 åˆ° 10
NUM_WORKERS = 8       

# æç¤ºè¯æ¨¡æ¿ (ä¿æŒä¸ä½ çš„ chat_pure_mlx.py ä¸€è‡´)
TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å›ç­”é—®é¢˜çš„ä¸“å®¶.
è¿™æ˜¯ä½ è¦å›ç­”çš„é—®é¢˜: {question}
è¯·åŸºäºä»¥ä¸Šé—®é¢˜ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„ç­”æ¡ˆ: {options}
"""

def worker_task(worker_id, data_chunk):
    """
    å­è¿›ç¨‹å·¥ä½œå‡½æ•°ï¼š
    æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½æ¨¡å‹ï¼Œå¤„ç†åˆ†é…åˆ°çš„æ•°æ®å—ã€‚
    """
    print(f"ğŸ”§ Worker {worker_id}: Loading model...")
    
    # âš ï¸ å…³é”®ï¼šæ¨¡å‹å¿…é¡»åœ¨å­è¿›ç¨‹å†…éƒ¨åŠ è½½ï¼Œä¸èƒ½åœ¨ä¸»è¿›ç¨‹åŠ è½½åä¼ é€’
    # trust_remote_code=True ä»¥é˜²ä¸‡ä¸€ï¼Œé€šå¸¸ mlx æ¨¡å‹ä¸éœ€è¦
    model, tokenizer = load(MODEL_ID, tokenizer_config={"trust_remote_code": True})
    
    results = []
    print(f"ğŸš€ Worker {worker_id}: Processing {len(data_chunk)} items...")
    
    start_t = time.time()
    
    for idx, item in enumerate(data_chunk):
        try:
            # 1. æ„é€  Prompt (ä¿æŒåŸæœ‰é€»è¾‘)
            prompt_content = TEMPLATE.format(
                question=item['question'],
                options=item['options']
            )
            
            # 2. åº”ç”¨èŠå¤©æ¨¡æ¿
            messages = [{"role": "user", "content": prompt_content}]
            prompt_text = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # 3. ç”Ÿæˆ
            # max_tokens=512 ä¿æŒä¸€è‡´
            response_text = generate(
                model, 
                tokenizer, 
                prompt=prompt_text, 
                max_tokens=512, 
                verbose=False
            )
            
            # 4. æ”¶é›†ç»“æœ
            results.append({
                'response': response_text,
                'answer': [item['answer_idx'], item['answer'], item.get('meta_info', '')]
            })
            
            # ç®€å•çš„è¿›åº¦æç¤º
            if (idx + 1) % 50 == 0:
                print(f"âœ… Worker {worker_id}: {idx + 1}/{len(data_chunk)} done")
                
        except Exception as e:
            print(f"âŒ Worker {worker_id} Error at item {idx}: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿä¿ç•™è®°å½•ï¼Œé¿å…æ•°æ®é”™ä½
            results.append({
                'response': "ERROR",
                'answer': [item['answer_idx'], item['answer'], item.get('meta_info', '')]
            })
            
    total_t = time.time() - start_t
    print(f"ğŸ Worker {worker_id} finished. Speed: {len(data_chunk)/total_t:.2f} it/s")
    return results

def main():
    # âš ï¸ å¿…é¡»è®¾ç½® spawnï¼Œå¦åˆ™ Metal ä¼šæŠ¥é”™
    multiprocessing.set_start_method('spawn', force=True)
    
    # 1. è¯»å–æ•°æ®
    print(f"ğŸ“¦ Loading data from {INPUT_FILE}...")
    questions = []
    if os.path.exists(INPUT_FILE):
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    questions.append(json.loads(line))
    else:
        print(f"âŒ Input file not found: {INPUT_FILE}")
        return

    total_items = len(questions)
    print(f"ğŸ“Š Total items: {total_items}. Launching {NUM_WORKERS} workers.")
    
    # 2. åˆ‡åˆ†æ•°æ®
    chunk_size = math.ceil(total_items / NUM_WORKERS)
    chunks = [questions[i:i + chunk_size] for i in range(0, total_items, chunk_size)]
    
    # å‡†å¤‡å‚æ•° [(id, chunk), (id, chunk), ...]
    tasks = [(i, chunk) for i, chunk in enumerate(chunks)]
    
    start_global = time.time()
    
    # 3. å¹¶è¡Œæ‰§è¡Œ
    with multiprocessing.Pool(processes=NUM_WORKERS) as pool:
        # starmap è‡ªåŠ¨è§£åŒ…å‚æ•°ä¼ ç»™ worker_task
        results_nested = pool.starmap(worker_task, tasks)
    
    # 4. åˆå¹¶ç»“æœ
    final_results = [item for sublist in results_nested for item in sublist]
    
    # 5. ä¿å­˜
    print(f"ğŸ’¾ Saving {len(final_results)} results to {OUTPUT_FILE}...")
    if os.path.exists(OUTPUT_FILE):
         os.rename(OUTPUT_FILE, f"{OUTPUT_FILE}.bak")
         
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for res in final_results:
            f.write(json.dumps(res, ensure_ascii=False) + '\n')
            
    total_time = time.time() - start_global
    print(f"\nğŸ‰ All Done! Total time: {total_time:.2f}s")
    print(f"âš¡ Aggregate Speed: {total_items/total_time:.2f} it/s")

if __name__ == "__main__":
    main()