import json
import os
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import nltk
from nltk.corpus import stopwords

from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.retrievers import EnsembleRetriever
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from sklearn.feature_extraction.text import CountVectorizer

# Project imports
from retrieval.bm25 import bm25retriever, chinese_tokenizer
from retrieval.bm25S import bm25sretriever
from preprocessing.clean import remove_symbols

# ==========================================
# ğŸ”§ æ ¸å¿ƒå·¥å…·ï¼šæ‰¹é‡è¿½åŠ å†™å…¥
# ==========================================
def append_batch_to_jsonl(data_list, filepath):
    """å°†ä¸€ä¸ªåˆ—è¡¨çš„æ•°æ®ä¸€æ¬¡æ€§è¿½åŠ å†™å…¥æ–‡ä»¶"""
    if not data_list:
        return

    # ä½¿ç”¨ 'a' (append) æ¨¡å¼
    with open(filepath, 'a', encoding='utf-8') as f:
        for data in data_list:
            # å¤„ç† LangChain å¯¹è±¡è½¬å­—ç¬¦ä¸²
            response_text = data['response']
            if hasattr(response_text, 'content'):
                response_text = response_text.content
            else:
                response_text = str(response_text)
            
            entry = {
                'response': response_text,
                'answer': data['answer']
            }
            json.dump(entry, f, ensure_ascii=False)
            f.write('\n')

def run_pipeline():
    # --- é…ç½® ---
    input_file = 'data_clean/questions/Mainland/test.jsonl'
    output_file = 'test_results.jsonl'
    
    # â±ï¸ æ‰¹é‡è®¾ç½®ï¼š50æ¡ â‰ˆ 2~3åˆ†é’Ÿ (å–å†³äºæ¨ç†é€Ÿåº¦)
    BATCH_SIZE = 50  
    
    # æ€§èƒ½å‚æ•°
    workers = 4             
    context_window = 4096   
    max_doc_length = 800    # æˆªæ–­é•¿åº¦

    # ==========================================
    # Phase 1: å‡†å¤‡å·¥ä½œ
    # ==========================================
    print("Step 1/4: Loading resources...")
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå…ˆé‡å‘½åå¤‡ä»½ï¼Œé˜²æ­¢è¿½åŠ åˆ°æ—§æ–‡ä»¶é‡Œ
    if os.path.exists(output_file):
        print(f"âš ï¸  Backup existing {output_file} -> {output_file}.bak")
        os.rename(output_file, f"{output_file}.bak")

    try:
        stopwords.words('chinese')
    except LookupError:
        nltk.download('stopwords')
    
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    raw_data = [json.loads(line) for line in lines]
    questions = [d['question'] for d in raw_data]
    print(f"Loaded {len(questions)} items.")

    print("Loading KeyBERT (MPS)...")
    embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='mps')
    kw_model = KeyBERT(embed_model)

    # ==========================================
    # Phase 2: æ‰¹é‡æå–å…³é”®è¯
    # ==========================================
    print("Step 2/4: Extracting keywords...")
    clean_questions = [remove_symbols(q) for q in questions]
    vectorizer = CountVectorizer(tokenizer=chinese_tokenizer)
    
    try:
        keywords_list = kw_model.extract_keywords(clean_questions, vectorizer=vectorizer, top_n=1)
        extracted_queries = [kws[0][0] if kws else clean_questions[i] for i, kws in enumerate(keywords_list)]
    except Exception as e:
        print(f"Extraction error: {e}. Using original queries.")
        extracted_queries = clean_questions

    del kw_model, embed_model 

    # ==========================================
    # Phase 3: æ£€ç´¢æ–‡æ¡£ (CPU)
    # ==========================================
    print("Step 3/4: Retrieving documents...")
    bm25 = bm25retriever(k=2)
    bm25s = bm25sretriever(k=2)
    ensemble = EnsembleRetriever(retrievers=[bm25, bm25s], weights=[0.5, 0.5])
    
    llm_inputs = []
    
    def retrieve_single(idx):
        try:
            query = extracted_queries[idx]
            docs = ensemble.invoke(query)
            # æˆªæ–­é€»è¾‘
            docs_text = "\n".join([d.page_content for d in docs])
            if len(docs_text) > max_doc_length:
                docs_text = docs_text[:max_doc_length] + "...(truncated)"
            
            return {
                "question": query,
                "reviews": docs_text,
                "options": str(raw_data[idx]['options']),
                "raw_data": raw_data[idx]
            }
        except Exception:
            return None

    with ThreadPoolExecutor(max_workers=8) as executor:
        results = list(tqdm(executor.map(retrieve_single, range(len(questions))), total=len(questions), desc="Retrieving"))
        llm_inputs = [r for r in results if r is not None]

    # ==========================================
    # Phase 4: æ‰¹é‡æ¨ç† + æ‰¹é‡å­˜æ¡£
    # ==========================================
    print(f"Step 4/4: Inference ({workers}Workers)...")
    print(f"ğŸ’¾ Saving every {BATCH_SIZE} items (approx 2 mins).")
    
    llm = OllamaLLM(
        model="qwen3:0.6b", 
        num_thread=workers,
        num_ctx=context_window,
        keep_alive="1h"
    )
    
    template = """
ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å›ç­”é—®é¢˜çš„ä¸“å®¶.
è¿™æ˜¯ä¸€äº›ç›¸å…³çš„èµ„æ–™: {reviews}
è¿™æ˜¯ä½ è¦å›ç­”çš„é—®é¢˜: {question}
è¯·åŸºäºä»¥ä¸Šèµ„æ–™å’Œé—®é¢˜ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„ç­”æ¡ˆ: {options}
"""
    chain = ChatPromptTemplate.from_template(template) | llm

    # ç¼“å†²åŒº
    results_buffer = []
    total_saved = 0

    def process_single(item):
        try:
            response = chain.invoke({
                "question": item['question'],
                "reviews": item['reviews'],
                "options": item['options']
            })
            return {
                'status': 'success',
                'response': response,
                'answer': [item['raw_data']['answer_idx'], item['raw_data']['answer'], item['raw_data']['meta_info']]
            }
        except Exception as e:
            return {'status': 'error', 'msg': str(e)}

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(process_single, item): i for i, item in enumerate(llm_inputs)}
        
        pbar = tqdm(as_completed(futures), total=len(futures), desc="Inferencing")
        
        for future in pbar:
            result = future.result()
            
            if result['status'] == 'success':
                # åŠ å…¥ç¼“å†²åŒº
                results_buffer.append(result)
            else:
                # é”™è¯¯å¶å°”æ‰“å°ä¸€ä¸‹ï¼Œä¸è¦å¤ªé¢‘ç¹
                pass 

            # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šç¼“å†²åŒºæ»¡äº†å°±è½ç›˜
            if len(results_buffer) >= BATCH_SIZE:
                append_batch_to_jsonl(results_buffer, output_file)
                total_saved += len(results_buffer)
                
                # æ‰“å°ä¸€ä¸ªå°æç¤ºï¼ˆä¸ä¼šé¢‘ç¹åˆ·å±ï¼‰
                pbar.write(f"âœ… Batch saved. Total: {total_saved}")
                
                # æ¸…ç©ºç¼“å†²åŒº
                results_buffer = []
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            pbar.set_postfix({"Buffer": len(results_buffer), "Saved": total_saved})

    # ==========================================
    # Phase 5: ä¿å­˜å‰©ä½™æ•°æ®
    # ==========================================
    if results_buffer:
        append_batch_to_jsonl(results_buffer, output_file)
        total_saved += len(results_buffer)
        print(f"âœ… Final batch saved.")

    print(f"\nAll Done! Total Saved: {total_saved} to {output_file}")

if __name__ == "__main__":
    run_pipeline()